name: Crawl_Build_Deploy

on:
  schedule:
    - cron: "0 5 * * *"   # 北京时间 13:00
  workflow_dispatch:

# 关键：为整条工作流授予 OIDC 与 Pages 权限
permissions:
  contents: read
  pages: write
  id-token: write

concurrency:
  group: "pages"
  cancel-in-progress: true

jobs:
  crawl_build_deploy:
    runs-on: ubuntu-latest

   # 可选：在 job 层再显式声明一次
      

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.9"

      - name: Install Python deps
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install feedparser jieba requests beautifulsoup4 lxml

      - name: Run Aggregator
        run: python -m aggregator.run

      - name: Build static site
        run: |
          mkdir -p output
          cp home.html output/index.html
          cp -r api output/api || true

      # 关键：先配置 Pages（会校验并生成所需元数据）
      - name: Configure Pages
        uses: actions/configure-pages@v5

      - name: Upload artifact for Pages
        uses: actions/upload-pages-artifact@v3
        with:
          path: output

      - name: Deploy to GitHub Pages
        uses: actions/deploy-pages@v4
