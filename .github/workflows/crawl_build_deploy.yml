name: Crawl_Build_Deploy

on:
  schedule:
    - cron: "0 5 * * *"  # 每天北京时间13:00运行
  workflow_dispatch:     # 手动触发支持

permissions:
  contents: read
  pages: write
  id-token: write

concurrency:
  group: "pages"
  cancel-in-progress: true

jobs:
  crawl_build_deploy:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.9"

      - name: Install Python deps
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install feedparser jieba requests beautifulsoup4 lxml

      - name: Run Aggregator
        run: python -m aggregator.run

      - name: Build static site
        run: |
          mkdir -p output
          cp home.html output/index.html
          cp -r api output/api || true
          echo "Build completed."

      - name: Upload artifact for Pages
        uses: actions/upload-pages-artifact@v3
        with:
          path: output

      - name: Deploy to GitHub Pages
        id: deploy
        uses: actions/deploy-pages@v4
